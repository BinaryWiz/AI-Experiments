{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just get the images we need\n",
    "style_img = Image.open('images/african_art.jpg')\n",
    "\n",
    "# Remember, numpy arrays of images is height x width x channels (RGB)\n",
    "style_array = np.array(style_img)\n",
    "\n",
    "# Make the channels first \n",
    "#style_array = np.rollaxis(style_array, 2, 0)\n",
    "\n",
    "# Do the same with the content image\n",
    "content_img = Image.open('images/owl_on_branch.jpg')\n",
    "content_array = np.array(content_img)\n",
    "\n",
    "# Define a variable to storing the shape of the images\n",
    "SHAPE = content_array.shape\n",
    "\n",
    "# Make channels first for content array\n",
    "#content_array = np.rollaxis(content_array, 2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the images\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(content_img)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(style_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shows images when the channel is first\n",
    "def imshow_channels_first(image):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(np.rollaxis(image[0], 0, 3))\n",
    "\n",
    "def imshow_channels_last(image):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(image[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the image\n",
    "def normalization(image):\n",
    "    vgg_means = [123.68, 116.779, 103.94]\n",
    "    ret_img = image.astype(dtype=np.float32)\n",
    "    ret_img[:, :, 0] = ret_img[:, :, 0] - vgg_means[0]\n",
    "    ret_img[:, :, 1] = ret_img[:, :, 1] - vgg_means[1]\n",
    "    ret_img[:, :, 2] = ret_img[:, :, 2] - vgg_means[2]\n",
    "    return ret_img\n",
    "\n",
    "# Add a 1 to the beginning of the matrix to act as the batch size\n",
    "norm_content_array = np.expand_dims(normalization(content_array), axis=0)\n",
    "norm_style_array = np.expand_dims(normalization(style_array), axis=0)\n",
    "\n",
    "# Show the images\n",
    "imshow_channels_last(norm_content_array)\n",
    "imshow_channels_last(norm_style_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(image):\n",
    "    vgg_means = [123.68, 116.779, 103.94]\n",
    "    ret_img = image.astype(dtype=np.float32)\n",
    "    ret_img[:, :, :, 0] = ret_img[:, :, :, 0] + vgg_means[0]\n",
    "    ret_img[:, :, :, 1] = ret_img[:, :, :, 1] + vgg_means[1]\n",
    "    ret_img[:, :, :, 2] = ret_img[:, :, :, 2] + vgg_means[2]\n",
    "    return ret_img.astype(dtype=np.uint)\n",
    "\n",
    "imshow_channels_last(denormalize(norm_content_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test that VGG19 works\n",
    "vgg_test = tf.keras.applications.VGG19(include_top=True, weights='imagenet')\n",
    "prediction_probabilities = vgg_test(tf.image.resize(norm_content_array, (224, 224)))\n",
    "prediction_probabilities.shape\n",
    "\n",
    "predicted_top_5 = tf.keras.applications.vgg19.decode_predictions(prediction_probabilities.numpy())[0]\n",
    "[(class_name, prob) for (number, class_name, prob) in predicted_top_5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the VGG19 model\n",
    "vgg = tf.keras.applications.VGG19(include_top = False, weights = \"imagenet\", pooling = \"avg\", input_shape = SHAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layers to use when computing the cost\n",
    "CONTENT_LAYERS = [\n",
    "    ('block4_conv2', 0.2),\n",
    "]\n",
    "\n",
    "STYLE_LAYERS = [\n",
    "    ('block1_conv1', 0.2),\n",
    "    ('block2_conv1', 0.2),\n",
    "    ('block3_conv1', 0.2),\n",
    "    ('block4_conv1', 0.2),\n",
    "    ('block5_conv1', 0.2)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model for \n",
    "def vgg_model_output(layers):\n",
    "    layer_outputs = []\n",
    "    for layer, weight in layers:\n",
    "        layer_outputs.append(vgg.get_layer(layer).output)\n",
    "    return tf.keras.Model([vgg.input], layer_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the variables for the style hidden layers and the content hidden layers\n",
    "content_activations = vgg_model_output(CONTENT_LAYERS)(norm_content_array)\n",
    "style_activations = vgg_model_output(STYLE_LAYERS)(norm_style_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise_to_img(image):\n",
    "    mean = 20.0\n",
    "    std = 10.0\n",
    "    return image + np.random.normal(mean, std, image.shape)\n",
    "\n",
    "imshow_channels_last(add_noise_to_img(norm_content_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG_Prop(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(VGG_Prop, self).__init__()\n",
    "        # Define the VGG19 model\n",
    "        self.vgg = tf.keras.applications.VGG19(include_top = False, weights = \"imagenet\", pooling = \"avg\", input_shape = SHAPE)\n",
    "        self.vgg.trainable = False\n",
    "        self.style_activations = style_activations\n",
    "        self.content_activations = content_activations\n",
    "        \n",
    "    # Create a model for \n",
    "    def vgg_model_output(self, layers):\n",
    "        layer_outputs = []\n",
    "        for layer, weight in layers:\n",
    "            layer_outputs.append(self.vgg.get_layer(layer).output)\n",
    "        return tf.keras.Model([self.vgg.input], layer_outputs)\n",
    "    \n",
    "    def forward_pass_vgg_content(self):\n",
    "        return self.vgg_model_output(CONTENT_LAYERS)(self.gen_img)\n",
    "    \n",
    "    def forward_pass_vgg_style(self):\n",
    "        return self.vgg_model_output(STYLE_LAYERS)(self.gen_img)\n",
    "    \n",
    "    def compute_content_cost(self):\n",
    "        # Compute the content cost \n",
    "        # * Can confirm this works\n",
    "        gen_img_content_activations = self.forward_pass_vgg_content()\n",
    "        gen_img_content_activations.trainable = False\n",
    "        m, n_H, n_W, n_C = gen_img_content_activations.get_shape().as_list()\n",
    "        \n",
    "        unrolled_content_activations = tf.reshape(self.content_activations, [m, n_W * n_H, n_C])\n",
    "        unrolled_gen_img_content_activations = tf.reshape(gen_img_content_activations, [m, n_W * n_H, n_C])\n",
    "        \n",
    "        cost = (1/(4 * n_W * n_H * n_C)) * tf.math.reduce_sum(tf.math.square(tf.subtract(unrolled_content_activations, unrolled_gen_img_content_activations)))\n",
    "        print('content cost ' + str(cost))\n",
    "        return cost\n",
    "        \n",
    "    def compute_gram(self, matrix):\n",
    "        return tf.linalg.matmul(matrix, matrix, transpose_b=True)\n",
    "    \n",
    "    def style_layer_cost(self, gen_img_layer_activation, style_layer_activation):\n",
    "        # Retrieve the dimensions from the activations\n",
    "        m, n_H, n_W, n_C = gen_img_layer_activation.get_shape().as_list()\n",
    "        \n",
    "        gen_img_layer_activation = tf.reshape(tf.transpose(gen_img_layer_activation, perm=[3, 1, 2, 0]), shape=[n_C, -1])\n",
    "        style_layer_activation = tf.reshape(tf.transpose(style_layer_activation, perm=[3, 1, 2, 0]), shape=[n_C, -1])\n",
    "        \n",
    "        gen_gram = self.compute_gram(gen_img_layer_activation)\n",
    "        style_gram = self.compute_gram(style_layer_activation)\n",
    "        \n",
    "        return (1/(4 * n_C**2 * (n_H * n_W)**2)) * tf.math.reduce_sum(tf.math.square(tf.math.subtract(style_gram, gen_gram)))\n",
    "    \n",
    "    def style_total_cost(self):\n",
    "        gen_img_style_activations = self.forward_pass_vgg_style()\n",
    "        total_cost = 0.0\n",
    "        for layer in range(0, len(gen_img_style_activations)):\n",
    "            total_cost = tf.add(total_cost, tf.math.multiply(STYLE_LAYERS[layer][1], self.style_layer_cost(gen_img_style_activations[layer], self.style_activations[layer])))\n",
    "        print('style cost' + str(total_cost))\n",
    "        return total_cost\n",
    "    \n",
    "    def total_cost(self, alpha = 10.0, beta = 40.0):\n",
    "        return tf.math.add(tf.math.multiply(alpha, self.compute_content_cost()), tf.math.multiply(beta, self.style_total_cost()))\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        self.gen_img = tf.Variable(add_noise_to_img(inputs), name=\"gen_img\", dtype=tf.float32, trainable=True)\n",
    "        return self.total_cost()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG_Prop()\n",
    "gen_img = norm_content_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(model):\n",
    "    return model(gen_img)\n",
    "\n",
    "def grad(model):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss(model)\n",
    "    return tape.gradient(loss_value, model.variables), loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(learning_rate=0.02)\n",
    "steps = 100\n",
    "for i in range(steps):\n",
    "    grads, loss_value = grad(model)\n",
    "    opt.apply_gradients(zip(grads, [model.gen_img]))\n",
    "    print(\"Loss at step \" + str(i) + \" = \" + str(loss_value))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
