{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input, Dropout\n",
    "\n",
    "# Have to download the stopwords\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "# Get the fasttext model (we are using the largest one they offer [600B tokens])\n",
    "fasttext_model = fasttext.load_model('models/crawl-300d-2M-subword.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", '!', '”', '#', '$', '%', '&', '’', '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~', ' ', 'null']\n"
     ]
    }
   ],
   "source": [
    "# Creates the stopwords\n",
    "to_stop = stopwords.words('english')\n",
    "punctuation = \"!”#$%&’()*+,-./:;<=>?@[\\]^_`{|}~ \"\n",
    "for c in punctuation:\n",
    "    to_stop.append(c)\n",
    "\n",
    "to_stop.append('null')\n",
    "print(to_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organizing and normalizing the data\n",
    "\"\"\"\n",
    "Essentially, we want to only have three attributes for each training example: title_one, title_two, label\n",
    "For normalization, we are just going to use the nltk stopwords and punctuation\n",
    "\"\"\"\n",
    "\n",
    "def preprocessing(orig_data):\n",
    "    \"\"\"\n",
    "    Normalizes the data by getting rid of stopwords and punctuation\n",
    "    \"\"\"\n",
    "    \n",
    "    # The new names of the columns\n",
    "    column_names = ['title_one', 'title_two', 'label']\n",
    "    # A new dataframe for the data we are going to be creating\n",
    "    norm_computers = pd.DataFrame(columns = column_names)\n",
    "    # Iterate over the original dataframe (I know it is slow and there are probably better ways to do it)\n",
    "    for row in orig_data.itertuples():\n",
    "        title_left = row.title_left.split(' ')\n",
    "        title_right = row.title_right.split(' ')\n",
    "        \n",
    "        # Creates a new list of only elements that are not in the stop words\n",
    "        temp_title_left = []\n",
    "        for word in title_left:\n",
    "            if word not in to_stop:\n",
    "                temp_title_left.append(word)\n",
    "                \n",
    "        # Creates a new list of only elements that are not in the stop words\n",
    "        temp_title_right = []\n",
    "        for word in title_right:\n",
    "            if word not in to_stop:\n",
    "                temp_title_right.append(word)\n",
    "        \n",
    "        # Join the elements in the list to create the strings\n",
    "        title_left = ' '.join(temp_title_left)\n",
    "        title_right = ' '.join(temp_title_right)\n",
    "        \n",
    "        # Append the newly created row (title_left, title_right, label) to the new dataframe\n",
    "        norm_computers = norm_computers.append(pd.DataFrame([[title_left, title_right, row.label]], columns=column_names))\n",
    "        \n",
    "    return norm_computers\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_simple_data():\n",
    "    \"\"\"\n",
    "    Creates and saves a simpler version of the original data that only contains the the two titles and the label.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the dataset of computer parts\n",
    "    computers_df = pd.read_json('data/computers_train/computers_train_xlarge_normalized.json.gz',compression='gzip', lines=True)\n",
    "    norm_computers = preprocessing(computers_df)\n",
    "    \n",
    "    # Save the new normalized and simplified data to a CSV file to load later\n",
    "    norm_computers.to_csv('data/computers_train/computers_train_xlarge_norm_simple.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "computer_df = pd.read_csv('data/computers_train/computers_train_xlarge_norm_simple.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "computer_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Info\n",
    "\n",
    "For the model, we are going to use LSTMs with a Constrastive Loss Function \n",
    "that will also be used to predict whether the two products are the same \n",
    "\n",
    "First, we have to convert the titles to embeddings through FastText before feeding into the LSTM.\n",
    "The embedding part of this model will not be a layer because:\n",
    "* The fasttext model would be time consuming and annoying to get to work with an embedding layer in Keras\n",
    "* The fasttext model is not going to be getting its embeddings optimized, so there is really no point in adding it as an embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def siamese_network(input_shape):\n",
    "    # Defines our inputs\n",
    "    left_title = Input(input_shape, dtype='float32')\n",
    "    right_title = Input(input_shape, dtype='float32')\n",
    "    \n",
    "    # The LSTM units\n",
    "    model = tf.keras.Sequential(name='siamese_network')\n",
    "    model.add(LSTM(units=256, return_sequences=True, name='lstm_1'))\n",
    "    model.add(Dropout(rate=0.3))\n",
    "    model.add(LSTM(units=128, return_sequences=True, name='lstm_2'))\n",
    "    model.add(Dropout(rate=0.3))\n",
    "    model.add(LSTM(units=128, name='lstm_3'))\n",
    "    model.add(Dropout(rate=0.3))\n",
    "    \n",
    "    # The dense layers\n",
    "    model.add(Dense(units=1024, activation='relu', name='dense_1'))\n",
    "    model.add(Dropout(rate=0.3))\n",
    "    model.add(Dense(units=512, activation='relu', name='dense_2'))\n",
    "    \n",
    "    # Forward propagate through the model to generate the encodings\n",
    "    encoded_left_title = model(left_title)\n",
    "    encoded_right_title = model(right_title)\n",
    "    \n",
    "    # Create and return the network\n",
    "    siamese_net = tf.keras.Model(inputs=[left_title, right_title], outputs=[encoded_left_title, encoded_right_title])\n",
    "    return siamese_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_15 (InputLayer)           [(None, 10, 300)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_16 (InputLayer)           [(None, 10, 300)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "siamese_network (Sequential)    (None, 512)          1555968     input_15[0][0]                   \n",
      "                                                                 input_16[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 1,555,968\n",
      "Trainable params: 1,555,968\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "max_len = 10\n",
    "EMBEDDING_SIZE = 300\n",
    "model = siamese_network((max_len, EMBEDDING_SIZE))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
