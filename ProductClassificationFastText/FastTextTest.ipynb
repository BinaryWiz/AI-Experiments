{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input, Dropout\n",
    "\n",
    "# Have to download the stopwords\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "# Get the fasttext model (we are using the largest one they offer [600B tokens])\n",
    "fasttext_model = fasttext.load_model('models/crawl-300d-2M-subword.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", '!', '”', '#', '$', '%', '&', '’', '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~', ' ', 'null']\n"
     ]
    }
   ],
   "source": [
    "# Creates the stopwords\n",
    "to_stop = stopwords.words('english')\n",
    "punctuation = \"!”#$%&’()*+,-./:;<=>?@[\\]^_`{|}~ \"\n",
    "for c in punctuation:\n",
    "    to_stop.append(c)\n",
    "\n",
    "to_stop.append('null')\n",
    "print(to_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organizing and normalizing the data\n",
    "\"\"\"\n",
    "Essentially, we want to only have three attributes for each training example: title_one, title_two, label\n",
    "For normalization, we are just going to use the nltk stopwords and punctuation\n",
    "\"\"\"\n",
    "\n",
    "def preprocessing(orig_data):\n",
    "    \"\"\"\n",
    "    Normalizes the data by getting rid of stopwords and punctuation\n",
    "    \"\"\"\n",
    "    \n",
    "    # The new names of the columns\n",
    "    column_names = ['title_one', 'title_two', 'label']\n",
    "    # A new dataframe for the data we are going to be creating\n",
    "    norm_computers = pd.DataFrame(columns = column_names)\n",
    "    # Iterate over the original dataframe (I know it is slow and there are probably better ways to do it)\n",
    "    for row in orig_data.itertuples():\n",
    "        title_left = row.title_left.split(' ')\n",
    "        title_right = row.title_right.split(' ')\n",
    "        \n",
    "        # Creates a new list of only elements that are not in the stop words\n",
    "        temp_title_left = []\n",
    "        for word in title_left:\n",
    "            if word not in to_stop:\n",
    "                temp_title_left.append(word)\n",
    "                \n",
    "        # Creates a new list of only elements that are not in the stop words\n",
    "        temp_title_right = []\n",
    "        for word in title_right:\n",
    "            if word not in to_stop:\n",
    "                temp_title_right.append(word)\n",
    "        \n",
    "        # Join the elements in the list to create the strings\n",
    "        title_left = ' '.join(temp_title_left)\n",
    "        title_right = ' '.join(temp_title_right)\n",
    "        \n",
    "        # Append the newly created row (title_left, title_right, label) to the new dataframe\n",
    "        norm_computers = norm_computers.append(pd.DataFrame([[title_left, title_right, row.label]], columns=column_names))\n",
    "        \n",
    "    return norm_computers\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_simple_data():\n",
    "    \"\"\"\n",
    "    Creates and saves a simpler version of the original data that only contains the the two titles and the label.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the dataset of computer parts\n",
    "    computers_df = pd.read_json('data/computers_train/computers_train_xlarge_normalized.json.gz',compression='gzip', lines=True)\n",
    "    norm_computers = preprocessing(computers_df)\n",
    "    \n",
    "    # Save the new normalized and simplified data to a CSV file to load later\n",
    "    norm_computers.to_csv('data/computers_train/computers_train_xlarge_norm_simple.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "computer_df = pd.read_csv('data/computers_train/computers_train_xlarge_norm_simple.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_one</th>\n",
       "      <th>title_two</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hp intel xeon x5560 prijzen tweakers</td>\n",
       "      <td>495906 b21 hp x5560 2 80ghz ml350 g6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>495906 b21 hp x5560 2 80ghz ml350 g6 new whole...</td>\n",
       "      <td>495906 b21 hp x5560 2 80ghz ml350 g6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>asus motherboard lga2066 ddr4 2 u atx 2xgbe pr...</td>\n",
       "      <td>asus prime x299 deluxe</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>asus prime x299 deluxe prijzen tweakers</td>\n",
       "      <td>asus prime x299 deluxe socket 2066 intel atx m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>asus prime x299 deluxe</td>\n",
       "      <td>asus prime x299 deluxe socket lga2066 motherbo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           title_one  \\\n",
       "0               hp intel xeon x5560 prijzen tweakers   \n",
       "1  495906 b21 hp x5560 2 80ghz ml350 g6 new whole...   \n",
       "2  asus motherboard lga2066 ddr4 2 u atx 2xgbe pr...   \n",
       "3            asus prime x299 deluxe prijzen tweakers   \n",
       "4                             asus prime x299 deluxe   \n",
       "\n",
       "                                           title_two  label  \n",
       "0               495906 b21 hp x5560 2 80ghz ml350 g6      1  \n",
       "1               495906 b21 hp x5560 2 80ghz ml350 g6      1  \n",
       "2                             asus prime x299 deluxe      1  \n",
       "3  asus prime x299 deluxe socket 2066 intel atx m...      1  \n",
       "4  asus prime x299 deluxe socket lga2066 motherbo...      1  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computer_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Info\n",
    "\n",
    "For the model, we are going to use LSTMs with a Constrastive Loss Function \n",
    "that will also be used to predict whether the two products are the same \n",
    "\n",
    "First, we have to convert the titles to embeddings through FastText before feeding into the LSTM.\n",
    "The embedding part of this model will not be a layer because:\n",
    "* The fasttext model would be time consuming and annoying to get to work with an embedding layer in Keras\n",
    "* The fasttext model is not going to be getting its embeddings optimized, so there is really no point in adding it as an embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def siamese_network(input_shape):\n",
    "    # Defines our inputs\n",
    "    left_title = Input(input_shape, dtype='float32')\n",
    "    right_title = Input(input_shape, dtype='float32')\n",
    "    \n",
    "    # The LSTM units\n",
    "    model = tf.keras.Sequential(name='siamese_network')\n",
    "    model.add(LSTM(units=256, return_sequences=True, name='lstm_1'))\n",
    "    model.add(Dropout(rate=0.3))\n",
    "    model.add(LSTM(units=128, return_sequences=True, name='lstm_2'))\n",
    "    model.add(Dropout(rate=0.3))\n",
    "    model.add(LSTM(units=128, name='lstm_3'))\n",
    "    model.add(Dropout(rate=0.3))\n",
    "    \n",
    "    # The dense layers\n",
    "    model.add(Dense(units=1024, activation='relu', name='dense_1'))\n",
    "    model.add(Dropout(rate=0.3))\n",
    "    model.add(Dense(units=512, activation='relu', name='dense_2'))\n",
    "    \n",
    "    # Forward propagate through the model to generate the encodings\n",
    "    encoded_left_title = model(left_title)\n",
    "    encoded_right_title = model(right_title)\n",
    "    \n",
    "    # Create and return the network\n",
    "    siamese_net = tf.keras.Model(inputs=[left_title, right_title], outputs=[encoded_left_title, encoded_right_title])\n",
    "    return siamese_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_15 (InputLayer)           [(None, 10, 300)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_16 (InputLayer)           [(None, 10, 300)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "siamese_network (Sequential)    (None, 512)          1555968     input_15[0][0]                   \n",
      "                                                                 input_16[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 1,555,968\n",
      "Trainable params: 1,555,968\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "max_len = 10\n",
    "EMBEDDING_SIZE = 300\n",
    "model = siamese_network((max_len, EMBEDDING_SIZE))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_len():\n",
    "    max_len = 0\n",
    "    for row in computer_df.itertuples():\n",
    "        if len(row.title_one.split(' ')) > max_len:\n",
    "            max_len = len(row.title_one.split(' '))\n",
    "            \n",
    "        if len(row.title_two.split(' ')) > max_len:\n",
    "            max_len = len(row.title_two.split(' '))\n",
    "    \n",
    "    return max_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAX_LEN: 42 EMBEDDING_SHAPE: (300,) m: 68461\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Definitions of some sizes in the training set\n",
    "\"\"\"\n",
    "# Returns the max word length in the entire training set\n",
    "MAX_LEN = get_max_len()\n",
    "EMBEDDING_SHAPE = fasttext_model['queen'].shape\n",
    "m = len(computer_df)\n",
    "print('MAX_LEN: ' + str(MAX_LEN), 'EMBEDDING_SHAPE: ' + str(EMBEDDING_SHAPE), 'm: ' + str(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create the numpy files of all the training embedddings\n",
    "We will have two numpy files:\n",
    "1. The training/validation/test sets\n",
    "2. The labels\n",
    "\"\"\"\n",
    "\n",
    "def create_embeddings(df):\n",
    "    # Create the numpy arrays for storing the embeddings and labels\n",
    "    total_embeddings = np.zeros(shape=(m, 2, MAX_LEN, EMBEDDING_SHAPE[0]))\n",
    "    labels = np.zeros(shape=(m))\n",
    "    \n",
    "    # I know this is a terrible way of doing this, but iterate over the dataframe\n",
    "    # and generate the embeddings to add to the numpy array\n",
    "    for idx, row in enumerate(computer_df.itertuples()):\n",
    "        for word_idx, word in enumerate(row.title_one.split(' ')):\n",
    "            total_embeddings[idx, 0, word_idx] = fasttext_model[word]\n",
    "            \n",
    "        for word_idx, word in enumerate(row.title_two.split(' ')):\n",
    "            total_embeddings[idx, 1, word_idx] = fasttext_model[word]\n",
    "            \n",
    "        labels[idx] = row.label\n",
    "        \n",
    "    return total_embeddings, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_embeddings():\n",
    "    embeddings, labels = create_embeddings(computer_df)\n",
    "    with open('data/computers_numpy/embeddings.npy', 'wb') as f:\n",
    "        np.save(f, embeddings)\n",
    "\n",
    "    with open('data/computers_numpy/labels.npy', 'wb') as f:\n",
    "        np.save(f, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings_and_labels():\n",
    "    loaded_embeddings = None\n",
    "    labels = None\n",
    "    with open('data/computers_numpy/embeddings.npy', 'rb') as f:\n",
    "        loaded_embeddings = np.load(f)\n",
    "    \n",
    "    with open('data/computers_numpy/labels.npy', 'rb') as f:\n",
    "        labels = np.load(f)\n",
    "    \n",
    "    return loaded_embeddings, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (68461, 2, 42, 300) Labels shape: (68461,)\n"
     ]
    }
   ],
   "source": [
    "embeddings, labels = load_embeddings_and_labels()\n",
    "print('Embeddings shape: ' + str(embeddings.shape), 'Labels shape: ' + str(labels.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
